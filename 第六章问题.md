# 第六章

### 逻辑斯蒂回归

**逻辑斯蒂回归的形状参数γ越大，曲线会越陡峭还是越平滑？** **P91**

答：

​		越陡峭；

​		γ是指数的分母上$e^{\frac {x}{\gamma}}$，γ小，则x变化对函数值的影响越明显



**逻辑斯蒂回归的核心公式** **P92**

答：

​			用<u>置信度</u>之比作为概率
$$
P(Y=1|x)=\frac{exp(\omega \cdot x)}{1+exp(\omega \cdot x)}
$$

$$
P(Y=1|x)=\frac{1}{1+exp(\omega \cdot x)}
$$



**什么是对数几率(log odds)** **P92**

答：
$$
logit(p)=\log{\frac{p}{1-p}}
$$
​		逻辑斯蒂回归中，使用特征与参数作用来求取对数几率
$$
\log{\frac{p}{1-p}}=\omega \cdot x
$$
​		好处：

​				1、将0~$+\infty$映射到0~1

​				2、置信度，转化为概率

**如何极大似然估计模型参数？** **P93**

答：

​			先有似然函数，然后取使其最大的参数ω，称为极大似然估计
$$
P(Y=1|x)=\pi(x),\quad P(Y=0|x)=1-\pi(x)
$$
​			似然函数：$P(X|\omega)$。在假设条件下，看到当前数据集$T={(x_1,y_1),(x_2,y_2),...(x_N,y_N)}$的概率
$$
\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
​			求使该式最大的参数ω即可  

  

**多项逻辑斯蒂回归和一般逻辑斯蒂回归的区别？** **P94**

答：

​			Y的取值更多，扩展上文提到的核心公式即可
$$
P(Y=k|x)=\frac{exp(\omega_k \cdot x)}{1 + \sum_{k=1}^{K-1} exp(\omega_k \cdot x)} \quad k=1,2,...K-1
$$

$$
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(\omega_k \cdot x)}
$$





### 最大熵模型

**最大熵模型的定义** **P94**

答：

​			用最大熵原理去选择模型。

​			“不仅要满足已有的事实，而且在没有更多信息的地方，必须是等可能的”

​			直观理解：

​			已有的事实绝不能违背，未知的领域承认所有可能。

​			前一句强调了训练集上的拟合，后半句强调了泛化性。“训练集拟合”很多方法都做得到，但是“最大熵原理”是用来保证泛化性。  





**最大熵模型与逻辑斯蒂回归的异同？** 

答：

​			同：

​					都是求解$P(Y|X)$

​			异：

​					逻辑斯蒂回归仅仅对输入抽取特征。特征函数为$f(x)$

​					最大熵对输入和输出同时抽取特征。特征函数为$f(x,y)$			

​			
$$
P(Y|x,\omega) = \frac{\exp(\omega \cdot f(x,y))}{\sum_{y\in Dom(y)}\exp(\omega \cdot f(x,y))}
$$


来源 https://www.zhihu.com/question/24094554/answer/108271031

以多类logistic regression为例。

在一般的视角下，每条输入数据会被表示成一个n维向量x，而模型有n*(k-1)个权重。  $\omega_k \cdot x$代表被分类至k的置信度，有一个类的权重为全0，算出来置信度=1。

在最大熵模型的视角下，每条输入的n个“特征”与k个类别共同组成了nk个特征，模型中有n*k个权重。每个类别会触发nk个特征中的n个，然后$\omega_k\cdot x$代表置信度。

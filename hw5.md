### 5.1
第一轮：
$$
\begin{align}
&H_{A_1}(D)=-1/3*log2(1/3)*3=log2(3)=1.585\\
&g_R(D,A_1)=g(D,A_1)/H_{A_1}(D)=0.083/0.528=0.052\\
&H_{A_2}(D)=-(1/3*log2(1/3)+2/3*log2(2/3))=0.918\\
&g_R(D,A_2)=g(D,A_2)/H_{A_2}(D)=0.324/0.918=0.353\\
&H_{A_3}(D)=-(1/3*log2(1/3)+2/3*log2(2/3))=0.918\\
&g_R(D,A_3)=g(D,A_3)/H_{A_3}(D)=0.42/0.918=0.458\\
&H_{A_4}(D)=-(1/3*log2(1/3)+6/15*log2(6/15)+4/15*log2(4/15))=1.566\\
&g_R(D,A_4)=g(D,A_4)/H_{A_4}(D)=0.363/1.566=0.232
\end{align}
$$
$A_3$的信息增益比最大，所以选择$A_3$作为根节点的特征。由于$A_3$取值为“是”的样本点都属于正类，所以这部分数据集构成一个叶子结点，对$A_3$取值为“否”的样本点继续进行划分。

第二轮：
$$
\begin{align}
&H_{A_1}(D)=-(4/9*log2(4/9)+2/9*log2(2/9)+3/9*log2(3/9))=1.53\\
&g_R(D,A_1)=g(D,A_1)/H_{A_1}(D)=0.251/1.53=0.164\\
&H_{A_2}(D)=-(1/3*log2(1/3)+2/3*log2(2/3))=0.918\\
&g_R(D,A_2)=g(D,A_2)/H_{A_2}(D)=0.918/0.918=1\\
&H_{A_2}(D)=-(4/9*log2(4/9)*2+1/9*log2(1/9))=1.392\\
&g_R(D,A_2)=g(D,A_2)/H_{A_2}(D)=0.474/1.392=0.341\\
\end{align}
$$
$A_2$的信息增益比最大，所以选择$A_2$作为结点的特征。此时，所有样本点都被正确分类，算法结束。生成的决策树与图5.5相同。



### 5.2
对划分出的每个区间而言，最优值即为区间内所有点的y值的平均值，因此平方误差等价于$方差\times 样本点数$ 。以s为切分点时的损失函数记为L(x=s)，y值序列记为a，集合A的方差记为D(A)。
$$
\begin{align}
&L(x=1)=D(a[:1])*1+D(a[1:])*9=22.648\\
&L(x=2)=D(a[:2])*2+D(a[2:])*8=17.702\\
&L(x=3)=12.19\\
&L(x=4)=7.379\\
&L(x=5)=3.359\\
&L(x=6)=5.074\\
&L(x=7)=10.052\\
&L(x=8)=15.178\\
&L(x=9)=21.329\\
&L(x=10)=27.932
\end{align}以x=5为切分点时损失函数最小，此时两类的最优值分别为5.060和8.176。
$$

以x=5为切分点时损失函数最小，此时两类的最优值分别为5.060和8.176。



